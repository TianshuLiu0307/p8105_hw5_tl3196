---
title: "p8105_hw5_tl3196"
author: "Tianshu Liu"
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
	message = FALSE, 
  dpi = 300,
  out.width = "90%"
)
theme_set(theme_minimal() + theme(legend.position = "bottom"))
options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)
scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

Import and tidy data from csv files.

```{r import_tidy_data}
# generate file list
file_path = "./data/con_exp_data/"
file_list = list.files(path = file_path, full.names = FALSE)

# import and tidy data
con_exp_df =
  expand_grid(
    sample = file_list
  ) %>% 
  mutate(
    data = purrr::map(
      .x = file_list, 
      ~read_csv(file = str_c(file_path, .x))),
    sample = str_replace(sample, ".csv", "")
  ) %>% 
   separate(
     col = sample, sep = '_', 
     into = c("arm", "id"),
     remove = FALSE
  ) %>% 
  unnest(data) %>% 
  pivot_longer(
    cols = week_1:week_8,
    names_to = "week",
    names_prefix = "week_",
    values_to = "observation"
  )

con_exp_df
```

Make a spaghetti plot for the tidy data.

```{r spaghetti_plot}
# spaghetti plot
con_exp_df %>% 
  ggplot(aes(x = week, y = observation, group = sample, color = sample)) + 
  geom_point(aes(shape = arm), alpha = .7) + 
  geom_line(aes(linetype = arm), alpha = .7)
```

From the plot, we can identify that there are not significant difference between participants in the two arms at the beginning of the study. 
Participants in the experimental arm have significant increasing trend on observations, while participants in the control arm almost remain the same as the beginning. 
With this trend, the difference between the two arms becomes more and more significant. 
From the 7th week, all participants in the experimental arm have higher observation than all participants in the control arm.
Therefore, compared to the control arm, the experimental arm has significant increase in observations.

## Problem 2

Import data from `homicide-data.csv`.

```{r import_data}
homicide_df = read_csv("./data/homicide-data.csv")
homicide_df
```

The raw homicide dataset collected by the Washington Post covers ``r nrow(homicide_df)`` homicide cases in 50 large U.S. cities. Each homicide case is described by ``r ncol(homicide_df)`` variables, including ``r colnames(homicide_df)``. The variables are shown in the table below.

```{r data_description, echo = FALSE}
col_names = colnames(homicide_df)
col_types = c()
for (col in col_names){
  col_types = c(col_types, class(pull(homicide_df, col)))
}
description = c(
  "case id",
  "date of the homicide being reported",
  "last name of the victim",
  "first name of the victim",
  "race of the victim",
  "age of the victim",
  "gender of the victim",
  "city of the homicide",
  "state of the homicide",
  "latitude of the homicide's location",
  "longtitude of the homicide's location",
  "current arresting condition"
)

knitr::kable(
  tibble(col_names, col_types, description)
)
```


```{r city_state}
homicide_df = 
  homicide_df %>% mutate(city_state = str_c(city, ", ", state))

homicide_df
```

```{r summarize_by_city}
# summarize total number of homicide by cities
homicide_df %>% 
  group_by(city) %>% 
  summarise(
    n_homicide = n()
  )

# summarize number of unsolved homicide by cities
homicide_df %>% 
  filter(disposition %in% c("Closed without arrest", "Open/No arrest")) %>% 
  group_by(city) %>% 
  summarise(
    n_unsolved = n()
  ) 
```

Estimate the proportion unsolved homicides in Baltimore

```{r balt_prop_test}
balt_n_total = 
  homicide_df %>% 
  filter(city == "Baltimore") %>% 
  count() %>% 
  as.numeric()

balt_n_unsolved = 
  homicide_df %>% 
  filter(
    city == "Baltimore",
    disposition %in% c("Closed without arrest", "Open/No arrest")
  ) %>% 
  count() %>% 
  as.numeric()

balt_prop = prop.test(x = balt_n_unsolved, n = balt_n_total, conf.level = 0.95) 
balt_prop_df = 
  balt_prop %>% 
  broom::tidy()

estimate = pull(balt_prop_df, estimate)
conf_low = pull(balt_prop_df, conf.low)
conf_high = pull(balt_prop_df, conf.high)

tibble(
  city = "Baltimore",
  estimate, 
  ci = str_c("(", conf_low, ", ", conf_high, ")")
)
```

Iterate prop test for each city

```{r prop_test}
# check cities
homicide_df %>% 
  distinct(city_state, city) %>% 
  count(city) %>% 
  arrange(desc(n))

# There are TWO Tulsa's in two different state!!
# solution: exclude the Tulsa in AL

# iterate prop test for each city
prop_result_df = 
  homicide_df %>% 
  group_by(city_state) %>% 
  filter(city_state != "Tulsa, AL") %>% 
  summarise(
    n_homicide = n(),
    n_unsolved = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
  ) %>% 
  mutate(
    test_result = purrr::map2(.x = n_unsolved, .y = n_homicide, 
                              ~prop.test(x = .x, n = .y, conf.level = 0.95) %>% broom::tidy())
  ) %>% 
  unnest(test_result) %>% 
  mutate(
    ci = str_c("(", conf.low, ", ", conf.high, ")")
  ) %>% 
  select(city_state, estimate,conf.low, conf.high, ci)

prop_result_df
```

```{r plot}
# plot estimated and CIs for each city
prop_result_df %>% 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) %>% 
  ggplot(aes(x = city_state, y = estimate, color = city_state)) + 
  geom_point() + 
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) + 
  theme(legend.position = "none") + 
  theme(axis.text.x = element_text(size=6, angle=45, hjust = 1)) + 
  labs(
    x = "City",
    y = "Estimated Proportion",
    title = "Estimated Proportions and CIs of Unsloved Homicides for Cities"
  )
```

## Problem 3

Write a function to generate simulation data

```{r sim_function}
gen_data = function(n_sample, mu, sigma = 5) {
  
  rnorm(n_sample, mean = mu, sd = sigma)
  
}
```

Generate 5000 datasets (n = 30, mu = 0, sd = 5) following normal distribution

```{r gen_data_0}
# generate 5000 datasets (n = 30, mu = 0, sd = 5)
data_0 = 
  expand_grid(
    sample_size = 30,
    mu = 0,
    iteration = 1:5000
  ) %>% 
  mutate(
    sim_data = map2(.x = sample_size, .y = mu, ~gen_data(n_sample = .x, mu = .y)),
    t_test = map(.x = sim_data, ~t.test(x = .x, mu = 0, conf.level = 0.95) %>% broom::tidy()) 
  ) %>% 
  unnest(t_test) %>% 
  select(mu, estimate, p.value)

# summarize the number of rejections based on p-value 
stat_data_0 =
  data_0 %>% 
  summarise(
    mu = 0,
    n = n(),
    n_reject = sum(p.value < 0.05),
    proportion = n_reject / n
  )

stat_data_0
```

Generate data and conduct t-test for different mu

```{r gen_data_diff_mu}
# generate 5000 datasets for different mu (n = 30, mu = 1,2,3,4,5,6 , sd = 5)
data = 
  expand_grid(
    sample_size = 30,
    mu = 1:6,
    iteration = 1:5000
  ) %>% 
  mutate(
    sim_data = map2(.x = sample_size, .y = mu, ~gen_data(n_sample = .x, mu = .y)),
    t_test = map(.x = sim_data, ~t.test(x = .x, mu = 0, conf.level = 0.95) %>% broom::tidy()) 
  ) %>% 
  unnest(t_test) %>% 
  select(mu, estimate, p.value)

# summarize the number of rejections based on p-value 
stat_data = 
  data %>% 
  group_by(mu) %>% 
  summarise(
    n = n(),
    n_reject = sum(p.value < 0.05),
    proportion = n_reject / n
  ) %>% 
  rbind(stat_data_0) %>% 
  arrange(mu)

stat_data
```

Make a plot showing the proportion of times the null was rejected (the power of the test) on the y axis and the true mu on the x axis

```{r power_vs_mu_plot}
stat_data %>% 
  ggplot(aes(x = mu, y = proportion, color = mu)) + 
  geom_point() + 
  geom_line() + 
  theme(legend.position = "none") + 
  labs(
    x = "True Mean (Effect Size)",
    y = "proportion of rejecting null (the power of the test)",
    title = "Proportion of Rejection vs True Mean"
  )
```

As true mean (effect size) grows, the proportion of rejecting null (the power of the test) also increases and gradually approaches 1. The rate of the power's increase firstly becomes faster and gradually slower, presenting an overall S shaped curve. 

```{r mu_hat_vs_true}
all_avg_mu = 
  data %>% 
  rbind(data_0) %>% 
  group_by(mu) %>% 
  summarise(
    type = "average estimate mu hat",
    avg_est_mu = mean(estimate)
  )

rej_avg_mu = 
  data %>% 
  rbind(data_0) %>% 
  filter(p.value < 0.05) %>% 
  group_by(mu) %>% 
  summarise(
    type = "average estimate mu hat only in samples for which the null was rejecte",
    avg_est_mu = mean(estimate)
  )

all_avg_mu %>% 
  rbind(rej_avg_mu) %>% 
  ggplot(aes(x = mu, y = avg_est_mu, group = type, color = type)) + 
  geom_point(aes(shape = type)) + 
  geom_line(aes(linetype = type)) + 
  labs(
    x = "True Mean (Effect Size)",
    y = "Average Estimated Mean",
    title = "Average Estimated Mean vs True Mean"
  )
```

From the plot above, we can conclude that the average $\hat{\mu}$ of the total simulation dataset always approaches the true mean in value. 
The average $\hat{\mu}$ increases as the true mean (effect size) increases, and thus it presents almost a linear relationship between average $\hat{\mu}$ and the true mean.

The average $\hat{\mu}$ only in samples for which the null was rejected does not have such simple linear relationship. In the samples with small true mean, i.e. mu = 1, 2, the average $\hat{\mu}$ in samples rejecting the null is obviously higher than both the true mean and the average $\hat{\mu}$ of all samples. As the true mean (effect size) continues increasing, the average $\hat{\mu}$ in samples rejecting the null gradually approaches the true mean and the average $\hat{\mu}$ of all samples. 

This is because as true mean increases, the proportion of rejecting the null also increases. When taking sample average of $\hat{\mu}$ across tests for which the null is rejected, if the true mean is small, there will be a low proportion of rejecting the null, thus the average $\hat{\mu}$ among these rejecting samples can be away from the true mean; if the true mean is large, there will be a much higher proportion of rejecting the null, making the sample average of $\hat{\mu}$ across tests for which the null is rejected approximately equal to the true $\mu$.
